"use strict";(self.webpackChunkopenedv_wiki=self.webpackChunkopenedv_wiki||[]).push([[7018],{4823:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/08-49097afe6ab527f6c4472aa9b1de47bc.png"},5326:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/09-5f4dc20eebf899ebb2ca1e99e971eb34.png"},24708:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/10-9443aca6a2e4f565074bf95f690a99db.png"},28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>d});var i=s(96540);const a={},_=i.createContext(a);function r(e){const n=i.useContext(_);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(_.Provider,{value:n},e.children)}},29965:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>t,contentTitle:()=>r,default:()=>m,frontMatter:()=>_,metadata:()=>d,toc:()=>l});var i=s(74848),a=s(28453);const _={title:"\u4eba\u8138\u8bc6\u522b\u5b9e\u9a8c",sidebar_position:4},r="\u4eba\u8138\u8bc6\u522b\u5b9e\u9a8c",d={id:"Boards/DNK230D/example-ai/face_recognition",title:"\u4eba\u8138\u8bc6\u522b\u5b9e\u9a8c",description:"\u524d\u8a00",source:"@site/docs/Boards/01_DNK230D/example-ai/face_recognition.md",sourceDirName:"Boards/01_DNK230D/example-ai",slug:"/Boards/DNK230D/example-ai/face_recognition",permalink:"/docs/Boards/DNK230D/example-ai/face_recognition",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:4,frontMatter:{title:"\u4eba\u8138\u8bc6\u522b\u5b9e\u9a8c",sidebar_position:4},sidebar:"DNK230DSidebar",previous:{title:"\u4eba\u8138\u59ff\u6001\u68c0\u6d4b\u5b9e\u9a8c",permalink:"/docs/Boards/DNK230D/example-ai/face_pose"},next:{title:"\u4eba\u4f53\u68c0\u6d4b\u5b9e\u9a8c",permalink:"/docs/Boards/DNK230D/example-ai/person_detection"}},t={},l=[{value:"\u524d\u8a00",id:"\u524d\u8a00",level:2},{value:"AI\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd",id:"ai\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd",level:2},{value:"\u786c\u4ef6\u8bbe\u8ba1",id:"\u786c\u4ef6\u8bbe\u8ba1",level:2},{value:"\u4f8b\u7a0b\u529f\u80fd",id:"\u4f8b\u7a0b\u529f\u80fd",level:3},{value:"\u786c\u4ef6\u8d44\u6e90",id:"\u786c\u4ef6\u8d44\u6e90",level:3},{value:"\u539f\u7406\u56fe",id:"\u539f\u7406\u56fe",level:3},{value:"\u5b9e\u9a8c\u4ee3\u7801",id:"\u5b9e\u9a8c\u4ee3\u7801",level:2},{value:"\u4eba\u8138\u6ce8\u518c\u7a0b\u5e8f",id:"\u4eba\u8138\u6ce8\u518c\u7a0b\u5e8f",level:3},{value:"\u4eba\u8138\u8bc6\u522b\u7a0b\u5e8f",id:"\u4eba\u8138\u8bc6\u522b\u7a0b\u5e8f",level:3},{value:"\u8fd0\u884c\u9a8c\u8bc1",id:"\u8fd0\u884c\u9a8c\u8bc1",level:2},{value:"\u4eba\u8138\u4fe1\u606f\u6ce8\u518c",id:"\u4eba\u8138\u4fe1\u606f\u6ce8\u518c",level:3},{value:"\u4eba\u8138\u8bc6\u522b",id:"\u4eba\u8138\u8bc6\u522b",level:3}];function o(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"\u4eba\u8138\u8bc6\u522b\u5b9e\u9a8c",children:"\u4eba\u8138\u8bc6\u522b\u5b9e\u9a8c"})}),"\n",(0,i.jsx)(n.h2,{id:"\u524d\u8a00",children:"\u524d\u8a00"}),"\n",(0,i.jsx)(n.p,{children:"\u5728\u4e0a\u4e00\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u5b66\u4e60\u4e86\u5982\u4f55\u5728CanMV\u4e0b\u4f7f\u7528CanMV AI\u89c6\u89c9\u5f00\u53d1\u6846\u67b6\u548cMicroPython\u7f16\u7a0b\u65b9\u6cd5\u5b9e\u73b0\u4eba\u8138\u59ff\u6001\u68c0\u6d4b\u7684\u529f\u80fd\uff0c\u672c\u7ae0\u5c06\u901a\u8fc7\u4eba\u8138\u8bc6\u522b\u5b9e\u9a8c\uff0c\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528CanMV AI\u89c6\u89c9\u5f00\u53d1\u6846\u67b6\u548cMicroPython\u7f16\u7a0b\u5b9e\u73b0\u4eba\u8138\u7279\u5f81\u503c\u7684\u8bfb\u53d6\uff0c\u5e76\u901a\u8fc7\u4eba\u8138\u7279\u6027\u6bd4\u5bf9\u7684\u65b9\u6cd5\u5b9e\u73b0\u4eba\u8138\u8bc6\u522b\u3002\u672c\u5b9e\u9a8c\u4e00\u5171\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff0c\u4e00\u4e2a\u662f\u7528\u4e8e\u8fdb\u884c\u4eba\u8138\u7279\u5f81\u503c\u8bfb\u53d6\u5e76\u6ce8\u518c\u4eba\u8138\u4fe1\u606f\u5230\u6570\u636e\u5e93\u7684\u4eba\u8138\u6ce8\u518c\u7a0b\u5e8f\uff0c\u53e6\u4e00\u90e8\u5206\u662f\u6839\u636e\u5df2\u6709\u7684\u4eba\u8138\u7279\u5f81\u4fe1\u606f\u8fdb\u884c\u4eba\u8138\u533a\u5206\u7684\u4eba\u8138\u8bc6\u522b\u7a0b\u5e8f\uff0c\u4eba\u8138\u6ce8\u518c\u662f\u4eba\u8138\u8bc6\u522b\u7684\u524d\u7f6e\u4efb\u52a1\uff0c\u5bf9\u4eba\u8138\u6570\u636e\u5e93\u4e2d\u6bcf\u4e00\u5f20\u5305\u542b\u4eba\u8138\u7684\u56fe\u7247\u8fdb\u884c\u7279\u5f81\u5316\uff0c\u5e76\u5c06\u4eba\u8138\u7279\u5f81\u4ee5bin\u6587\u4ef6\u7684\u5f62\u5f0f\u5199\u5165\u4eba\u8138\u6570\u636e\u5e93\u76ee\u5f55\uff0c\u4ee5\u5907\u4eba\u8138\u8bc6\u522b\u7a0b\u5e8f\u8c03\u7528\u3002\u4eba\u8138\u8bc6\u522b\u5e94\u7528\u662f\u57fa\u4e8e\u4eba\u8138\u6ce8\u518c\u7684\u4fe1\u606f\u5bf9\u89c6\u9891\u4e2d\u7684\u6bcf\u4e00\u5e27\u56fe\u7247\u505a\u4eba\u8138\u8eab\u4efd\u8bc6\u522b\uff0c\u5982\u679c\u8bc6\u522b\u5230\u7684\u4eba\u8138\u5728\u6ce8\u518c\u6570\u636e\u5e93\u4e2d\uff0c\u5219\u6807\u6ce8\u8bc6\u522b\u4eba\u8138\u7684\u8eab\u4efd\u4fe1\u606f\uff0c\u5426\u5219\u663e\u793aunknown\u3002\u901a\u8fc7\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u5373\u53ef\u5bf9\u4eba\u8138\u7279\u5f81\u8fdb\u884c\u533a\u5206\uff0c\u4ece\u800c\u5b9e\u73b0\u4eba\u8138\u8bc6\u522b\u7684\u529f\u80fd\u3002\u6700\u540e\u6839\u636e\u8bc6\u522b\u7ed3\u679c\u5c06\u4eba\u8138\u4fe1\u606f\u7ed8\u5236\u5e76\u663e\u793a\u5230\u663e\u793a\u5668\u4e0a\u3002\u901a\u8fc7\u672c\u7ae0\u7684\u5b66\u4e60\uff0c\u8bfb\u8005\u5c06\u638c\u63e1\u5982\u4f55\u5728CanMV\u4e0b\u4f7f\u7528CanMV AI\u89c6\u89c9\u5f00\u53d1\u6846\u67b6\u548cMicroPython\u7f16\u7a0b\u65b9\u6cd5\u5b9e\u73b0\u4eba\u8138\u8bc6\u522b\u529f\u80fd\u3002"}),"\n",(0,i.jsx)(n.h2,{id:"ai\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd",children:"AI\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd"}),"\n",(0,i.jsxs)(n.p,{children:["\u4e3a\u4e86\u7b80\u5316AI\u5f00\u53d1\u6d41\u7a0b\u5e76\u964d\u4f4eAI\u5f00\u53d1\u96be\u5ea6\uff0cCanMV\u5b98\u65b9\u9488\u5bf9K230D\u4e13\u95e8\u642d\u5efa\u4e86AI\u5f00\u53d1\u6846\u67b6\uff0c\u6709\u5173AI\u5f00\u53d1\u6846\u67b6\u7684\u4ecb\u7ecd\uff0c\u8bf7\u89c1",(0,i.jsx)(n.a,{href:"/docs/Boards/DNK230D/example-ai/development_framework",children:"CanMV AI\u5f00\u53d1\u6846\u67b6"})]}),"\n",(0,i.jsx)(n.h2,{id:"\u786c\u4ef6\u8bbe\u8ba1",children:"\u786c\u4ef6\u8bbe\u8ba1"}),"\n",(0,i.jsx)(n.h3,{id:"\u4f8b\u7a0b\u529f\u80fd",children:"\u4f8b\u7a0b\u529f\u80fd"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:'\u4eba\u8138\u6ce8\u518c\u7a0b\u5e8f\uff0c\u5373main1.py\uff1a\u4ece"/sdcard/examples/utils/db_img/"\u8def\u5f84\u8bfb\u53d6\u6ce8\u518c\u4eba\u8138\u7684\u56fe\u7247\u8fdb\u884c\u4eba\u8138\u7279\u5f81\u503c\u63d0\u53d6\uff0c\u5e76\u5c06\u4eba\u8138\u7279\u5f81\u4fe1\u606f\u4ee5bin\u6587\u4ef6\u7684\u5f62\u5f0f\u5199\u5165\u4eba\u8138\u6570\u636e\u5e93\uff08\u76ee\u5f55\u8def\u5f84\u4e3a\uff1a"/sdcard/examples/utils/db/"\uff09\uff0c\u8fd0\u884c\u7ed3\u675f\u540e\u5373\u53ef\u4f9b\u4eba\u8138\u8bc6\u522b\u7a0b\u5e8f\u8c03\u7528\uff0c\u6240\u4ee5\u8fd0\u884c\u4eba\u8138\u8bc6\u522b\u7a0b\u5e8f\u524d\uff0c\u4e00\u5b9a\u8981\u5148\u8fd0\u884c\u4eba\u8138\u6ce8\u518c\u7a0b\u5e8f\u3002'}),"\n",(0,i.jsx)(n.li,{children:"\u4eba\u8138\u8bc6\u522b\u7a0b\u5e8f\uff0c\u5373main2.py\uff1a\u83b7\u53d6\u6444\u50cf\u5934\u8f93\u51fa\u7684\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u56fe\u50cf\u8f93\u5165\u5230CanMV K230D\u7684AI\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u672c\u5b9e\u9a8c\u4f7f\u7528\u4e86\u4e24\u4e2aAI\u6a21\u578b\uff0c\u4e00\u4e2a\u662f\u524d\u9762\u7ae0\u8282\u4f7f\u7528\u5230\u7684\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\uff0c\u53e6\u4e00\u4e2a\u662f\u7528\u4e8e\u5b9e\u73b0\u4eba\u8138\u7279\u5f81\u6bd4\u5bf9\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u3002\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8d1f\u8d23\u627e\u51fa\u56fe\u50cf\u4e2d\u7684\u4eba\u8138\u533a\u57df\uff0c\u7136\u540e\u5c06\u8be5\u533a\u57df\u4f20\u9012\u7ed9\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u8fdb\u884c\u4eba\u8138\u7279\u5f81\u503c\u63d0\u53d6\uff0c\u7136\u540e\u5c06\u63d0\u53d6\u7684\u4eba\u8138\u4e0e\u6ce8\u518c\u6570\u636e\u5e93\u7684\u6570\u636e\u8fdb\u884c\u6bd4\u5bf9\uff0c\u5982\u679c\u8bc6\u522b\u5230\u7684\u4eba\u8138\u5728\u6ce8\u518c\u6570\u636e\u5e93\u4e2d\uff0c\u5219\u6807\u6ce8\u8bc6\u522b\u4eba\u8138\u7684\u8eab\u4efd\u4fe1\u606f\uff0c\u5426\u5219\u663e\u793aunknown\uff0c\u6700\u540e\u5c06\u56fe\u50cf\u663e\u793a\u5728LCD\u4e0a\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"\u786c\u4ef6\u8d44\u6e90",children:"\u786c\u4ef6\u8d44\u6e90"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"\u672c\u7ae0\u5b9e\u9a8c\u5185\u5bb9\u4e3b\u8981\u8bb2\u89e3K230D\u7684\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668KPU\u7684\u4f7f\u7528\uff0c\u65e0\u9700\u5173\u6ce8\u786c\u4ef6\u8d44\u6e90\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"\u539f\u7406\u56fe",children:"\u539f\u7406\u56fe"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"\u672c\u7ae0\u5b9e\u9a8c\u5185\u5bb9\u4e3b\u8981\u8bb2\u89e3K230D\u7684\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668KPU\u7684\u4f7f\u7528\uff0c\u65e0\u9700\u5173\u6ce8\u539f\u7406\u56fe\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"\u5b9e\u9a8c\u4ee3\u7801",children:"\u5b9e\u9a8c\u4ee3\u7801"}),"\n",(0,i.jsx)(n.h3,{id:"\u4eba\u8138\u6ce8\u518c\u7a0b\u5e8f",children:"\u4eba\u8138\u6ce8\u518c\u7a0b\u5e8f"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom media.sensor import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\nimport math\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.image_size=[]\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.image_size=[input_image_size[1],input_image_size[0]]\n            # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u5e76\u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(ai2d_input_size), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.image_size,results)\n            if len(res)==0:\n                return res\n            else:\n                return res[0],res[1]\n\n    def get_pad_param(self,image_input_size):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / image_input_size[0]\n        ratio_h = dst_h / image_input_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * image_input_size[0])\n        new_h = (int)(ratio * image_input_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u6ce8\u518c\u4efb\u52a1\u7c7b\nclass FaceRegistrationApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u4eba\u8138\u6ce8\u518c\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u6807\u51c65\u5b98\n        self.umeyama_args_112 = [\n            38.2946 , 51.6963 ,\n            73.5318 , 51.5014 ,\n            56.0252 , 71.7366 ,\n            41.5493 , 92.3655 ,\n            70.7299 , 92.2041\n        ]\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86affine\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,landm,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97affine\u77e9\u9635\uff0c\u5e76\u8bbe\u7f6e\u4eff\u5c04\u53d8\u6362\u9884\u5904\u7406\n            affine_matrix = self.get_affine_matrix(landm)\n            self.ai2d.affine(nn.interp_method.cv2_bilinear,0, 0, 127, 1,affine_matrix)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            return results[0][0]\n\n    def svd22(self,a):\n        # svd\n        s = [0.0, 0.0]\n        u = [0.0, 0.0, 0.0, 0.0]\n        v = [0.0, 0.0, 0.0, 0.0]\n        s[0] = (math.sqrt((a[0] - a[3]) ** 2 + (a[1] + a[2]) ** 2) + math.sqrt((a[0] + a[3]) ** 2 + (a[1] - a[2]) ** 2)) / 2\n        s[1] = abs(s[0] - math.sqrt((a[0] - a[3]) ** 2 + (a[1] + a[2]) ** 2))\n        v[2] = math.sin((math.atan2(2 * (a[0] * a[1] + a[2] * a[3]), a[0] ** 2 - a[1] ** 2 + a[2] ** 2 - a[3] ** 2)) / 2) if \\\n        s[0] > s[1] else 0\n        v[0] = math.sqrt(1 - v[2] ** 2)\n        v[1] = -v[2]\n        v[3] = v[0]\n        u[0] = -(a[0] * v[0] + a[1] * v[2]) / s[0] if s[0] != 0 else 1\n        u[2] = -(a[2] * v[0] + a[3] * v[2]) / s[0] if s[0] != 0 else 0\n        u[1] = (a[0] * v[1] + a[1] * v[3]) / s[1] if s[1] != 0 else -u[2]\n        u[3] = (a[2] * v[1] + a[3] * v[3]) / s[1] if s[1] != 0 else u[0]\n        v[0] = -v[0]\n        v[2] = -v[2]\n        return u, s, v\n\n    def image_umeyama_112(self,src):\n        # \u4f7f\u7528Umeyama\u7b97\u6cd5\u8ba1\u7b97\u4eff\u5c04\u53d8\u6362\u77e9\u9635\n        SRC_NUM = 5\n        SRC_DIM = 2\n        src_mean = [0.0, 0.0]\n        dst_mean = [0.0, 0.0]\n        for i in range(0,SRC_NUM * 2,2):\n            src_mean[0] += src[i]\n            src_mean[1] += src[i + 1]\n            dst_mean[0] += self.umeyama_args_112[i]\n            dst_mean[1] += self.umeyama_args_112[i + 1]\n        src_mean[0] /= SRC_NUM\n        src_mean[1] /= SRC_NUM\n        dst_mean[0] /= SRC_NUM\n        dst_mean[1] /= SRC_NUM\n        src_demean = [[0.0, 0.0] for _ in range(SRC_NUM)]\n        dst_demean = [[0.0, 0.0] for _ in range(SRC_NUM)]\n        for i in range(SRC_NUM):\n            src_demean[i][0] = src[2 * i] - src_mean[0]\n            src_demean[i][1] = src[2 * i + 1] - src_mean[1]\n            dst_demean[i][0] = self.umeyama_args_112[2 * i] - dst_mean[0]\n            dst_demean[i][1] = self.umeyama_args_112[2 * i + 1] - dst_mean[1]\n        A = [[0.0, 0.0], [0.0, 0.0]]\n        for i in range(SRC_DIM):\n            for k in range(SRC_DIM):\n                for j in range(SRC_NUM):\n                    A[i][k] += dst_demean[j][i] * src_demean[j][k]\n                A[i][k] /= SRC_NUM\n        T = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n        U, S, V = self.svd22([A[0][0], A[0][1], A[1][0], A[1][1]])\n        T[0][0] = U[0] * V[0] + U[1] * V[2]\n        T[0][1] = U[0] * V[1] + U[1] * V[3]\n        T[1][0] = U[2] * V[0] + U[3] * V[2]\n        T[1][1] = U[2] * V[1] + U[3] * V[3]\n        scale = 1.0\n        src_demean_mean = [0.0, 0.0]\n        src_demean_var = [0.0, 0.0]\n        for i in range(SRC_NUM):\n            src_demean_mean[0] += src_demean[i][0]\n            src_demean_mean[1] += src_demean[i][1]\n        src_demean_mean[0] /= SRC_NUM\n        src_demean_mean[1] /= SRC_NUM\n        for i in range(SRC_NUM):\n            src_demean_var[0] += (src_demean_mean[0] - src_demean[i][0]) * (src_demean_mean[0] - src_demean[i][0])\n            src_demean_var[1] += (src_demean_mean[1] - src_demean[i][1]) * (src_demean_mean[1] - src_demean[i][1])\n        src_demean_var[0] /= SRC_NUM\n        src_demean_var[1] /= SRC_NUM\n        scale = 1.0 / (src_demean_var[0] + src_demean_var[1]) * (S[0] + S[1])\n        T[0][2] = dst_mean[0] - scale * (T[0][0] * src_mean[0] + T[0][1] * src_mean[1])\n        T[1][2] = dst_mean[1] - scale * (T[1][0] * src_mean[0] + T[1][1] * src_mean[1])\n        T[0][0] *= scale\n        T[0][1] *= scale\n        T[1][0] *= scale\n        T[1][1] *= scale\n        return T\n\n    def get_affine_matrix(self,sparse_points):\n        # \u83b7\u53d6affine\u53d8\u6362\u77e9\u9635\n        with ScopedTiming("get_affine_matrix", self.debug_mode > 1):\n            # \u4f7f\u7528Umeyama\u7b97\u6cd5\u8ba1\u7b97\u4eff\u5c04\u53d8\u6362\u77e9\u9635\n            matrix_dst = self.image_umeyama_112(sparse_points)\n            matrix_dst = [matrix_dst[0][0],matrix_dst[0][1],matrix_dst[0][2],\n                          matrix_dst[1][0],matrix_dst[1][1],matrix_dst[1][2]]\n            return matrix_dst\n\n# \u4eba\u8138\u6ce8\u518c\u4efb\u52a1\u7c7b\nclass FaceRegistration:\n    def __init__(self,face_det_kmodel,face_reg_kmodel,det_input_size,reg_input_size,database_dir,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u8138\u6ce8\u518c\u6a21\u578b\u8def\u5f84\n        self.face_reg_kmodel=face_reg_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u6ce8\u518c\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.reg_input_size=reg_input_size\n        self.database_dir=database_dir\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,debug_mode=0)\n        self.face_reg=FaceRegistrationApp(self.face_reg_kmodel,model_input_size=self.reg_input_size,rgb888p_size=self.rgb888p_size)\n\n    # run\u51fd\u6570\n    def run(self,input_np,img_file):\n        self.face_det.config_preprocess(input_image_size=[input_np.shape[3],input_np.shape[2]])\n        det_boxes,landms=self.face_det.run(input_np)\n        if det_boxes:\n            if det_boxes.shape[0] == 1:\n                # \u82e5\u662f\u53ea\u68c0\u6d4b\u5230\u4e00\u5f20\u4eba\u8138\uff0c\u5219\u5c06\u8be5\u4eba\u8138\u6ce8\u518c\u5230\u6570\u636e\u5e93\n                db_i_name = img_file.split(\'.\')[0]\n                for landm in landms:\n                    self.face_reg.config_preprocess(landm,input_image_size=[input_np.shape[3],input_np.shape[2]])\n                    reg_result = self.face_reg.run(input_np)\n                    with open(self.database_dir+\'{}.bin\'.format(db_i_name), "wb") as file:\n                        file.write(reg_result.tobytes())\n                        print(\'Success!\')\n            else:\n                print(\'Only one person in a picture when you sign up\')\n        else:\n            print(\'No person detected\')\n\n    def image2rgb888array(self,img):   #4\u7ef4\n        # \u5c06Image\u8f6c\u6362\u4e3argb888\u683c\u5f0f\n        with ScopedTiming("fr_kpu_deinit",self.debug_mode > 0):\n            img_data_rgb888=img.to_rgb888()\n            # hwc,rgb888\n            img_hwc=img_data_rgb888.to_numpy_ref()\n            shape=img_hwc.shape\n            img_tmp = img_hwc.reshape((shape[0] * shape[1], shape[2]))\n            img_tmp_trans = img_tmp.transpose()\n            img_res=img_tmp_trans.copy()\n            # chw,rgb888\n            img_return=img_res.reshape((1,shape[2],shape[0],shape[1]))\n        return  img_return\n\n\nif __name__=="__main__":\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/examples/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u6ce8\u518c\u6a21\u578b\u8def\u5f84\n    face_reg_kmodel_path="/sdcard/examples/kmodel/face_recognition.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/examples/utils/prior_data_320.bin"\n    database_dir="/sdcard/examples/utils/db/"               # \u4eba\u8138\u4fe1\u606f\u6ce8\u518c\u6570\u636e\u5b58\u653e\u8def\u5f84\n    database_img_dir="/sdcard/examples/utils/db_img/"       # \u9700\u8981\u6ce8\u518c\u7684\u4eba\u8138\u56fe\u7247\u5b58\u653e\u4f4d\u7f6e\n    face_det_input_size=[320,320]\n    face_reg_input_size=[112,112]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n    max_register_face = 100              #\u6570\u636e\u5e93\u6700\u591a\u4eba\u8138\u4e2a\u6570\n    feature_num = 128                    #\u4eba\u8138\u8bc6\u522b\u7279\u5f81\u7ef4\u5ea6\n\n    fr=FaceRegistration(face_det_kmodel_path,face_reg_kmodel_path,det_input_size=face_det_input_size,reg_input_size=face_reg_input_size,database_dir=database_dir,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold)\n    try:\n        # \u83b7\u53d6\u56fe\u50cf\u5217\u8868\n        img_list = os.listdir(database_img_dir)\n        for img_file in img_list:\n            #\u672c\u5730\u8bfb\u53d6\u4e00\u5f20\u56fe\u50cf\n            full_img_file = database_img_dir + img_file\n            print(full_img_file)\n            img = image.Image(full_img_file)\n            img.compress_for_ide()\n            # \u8f6crgb888\u7684chw\u683c\u5f0f\n            rgb888p_img_ndarry = fr.image2rgb888array(img)\n            # \u4eba\u8138\u6ce8\u518c\n            fr.run(rgb888p_img_ndarry,img_file)\n            gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        fr.face_det.deinit()\n        fr.face_reg.deinit()\n'})}),"\n",(0,i.jsx)(n.p,{children:"\u53ef\u4ee5\u770b\u5230\u9996\u5148\u5b9a\u4e49\u4e86\u6a21\u578b\u8def\u5f84\u3001\u4eba\u8138\u6ce8\u518c\u7684\u56fe\u50cf\u6587\u4ef6\u8def\u5f84\u3001\u6ce8\u518c\u4eba\u8138\u6570\u636e\u5e93\u7684\u8def\u5f84\u7b49\uff0c\u7136\u540e\u8c03\u7528\u81ea\u5b9a\u4e49FaceRegistration\u7c7b\u6784\u5efa\u4eba\u8138\u59ff\u6001\u4efb\u52a1\u7c7b\uff0cFaceRegistration\u7c7b\u4f1a\u901a\u8fc7\u8c03\u7528FaceDetApp\u7c7b\u548cFaceRegistrationApp\u7c7b\u5b8c\u6210\u5bf9AIBase\u63a5\u53e3\u7684\u521d\u59cb\u5316\u4ee5\u53ca\u4f7f\u7528Ai2D\u63a5\u53e3\u7684\u65b9\u6cd5\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u548c\u4eba\u8138\u6ce8\u518c\u6a21\u578b\u8f93\u5165\u56fe\u50cf\u7684\u9884\u5904\u7406\u65b9\u6cd5\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u63a5\u7740\u4f9d\u6b21\u4ece/sdcard/examples/utils/db_img/\u8def\u5f84\u8bfb\u53d6\u9700\u8981\u6ce8\u518c\u7684\u4eba\u8138\u56fe\u50cf\u8fdb\u884c\u6ce8\u518c\uff0c\u7136\u540e\u6ce8\u518c\u7684\u4eba\u8138\u7279\u5f81\u4fe1\u606f\u4ee5bin\u6587\u4ef6\u7684\u5f62\u5f0f\u5b58\u653e\u4e8e\u6ce8\u518c\u4eba\u8138\u6570\u636e\u5e93\u4e2d\u3002"}),"\n",(0,i.jsx)(n.h3,{id:"\u4eba\u8138\u8bc6\u522b\u7a0b\u5e8f",children:"\u4eba\u8138\u8bc6\u522b\u7a0b\u5e8f"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom media.sensor import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\nimport math\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u5e76\u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.rgb888p_size,results)\n            if len(res)==0:\n                return res,res\n            else:\n                return res[0],res[1]\n\n    def get_pad_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / self.rgb888p_size[0]\n        ratio_h = dst_h / self.rgb888p_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * self.rgb888p_size[0])\n        new_h = (int)(ratio * self.rgb888p_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u6ce8\u518c\u4efb\u52a1\u7c7b\nclass FaceRegistrationApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u6807\u51c65\u5b98\n        self.umeyama_args_112 = [\n            38.2946 , 51.6963 ,\n            73.5318 , 51.5014 ,\n            56.0252 , 71.7366 ,\n            41.5493 , 92.3655 ,\n            70.7299 , 92.2041\n        ]\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86affine\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,landm,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97affine\u77e9\u9635\uff0c\u5e76\u8bbe\u7f6e\u4eff\u5c04\u53d8\u6362\u9884\u5904\u7406\n            affine_matrix = self.get_affine_matrix(landm)\n            self.ai2d.affine(nn.interp_method.cv2_bilinear,0, 0, 127, 1,affine_matrix)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            return results[0][0]\n\n    def svd22(self,a):\n        # svd\n        s = [0.0, 0.0]\n        u = [0.0, 0.0, 0.0, 0.0]\n        v = [0.0, 0.0, 0.0, 0.0]\n        s[0] = (math.sqrt((a[0] - a[3]) ** 2 + (a[1] + a[2]) ** 2) + math.sqrt((a[0] + a[3]) ** 2 + (a[1] - a[2]) ** 2)) / 2\n        s[1] = abs(s[0] - math.sqrt((a[0] - a[3]) ** 2 + (a[1] + a[2]) ** 2))\n        v[2] = math.sin((math.atan2(2 * (a[0] * a[1] + a[2] * a[3]), a[0] ** 2 - a[1] ** 2 + a[2] ** 2 - a[3] ** 2)) / 2) if \\\n        s[0] > s[1] else 0\n        v[0] = math.sqrt(1 - v[2] ** 2)\n        v[1] = -v[2]\n        v[3] = v[0]\n        u[0] = -(a[0] * v[0] + a[1] * v[2]) / s[0] if s[0] != 0 else 1\n        u[2] = -(a[2] * v[0] + a[3] * v[2]) / s[0] if s[0] != 0 else 0\n        u[1] = (a[0] * v[1] + a[1] * v[3]) / s[1] if s[1] != 0 else -u[2]\n        u[3] = (a[2] * v[1] + a[3] * v[3]) / s[1] if s[1] != 0 else u[0]\n        v[0] = -v[0]\n        v[2] = -v[2]\n        return u, s, v\n\n    def image_umeyama_112(self,src):\n        # \u4f7f\u7528Umeyama\u7b97\u6cd5\u8ba1\u7b97\u4eff\u5c04\u53d8\u6362\u77e9\u9635\n        SRC_NUM = 5\n        SRC_DIM = 2\n        src_mean = [0.0, 0.0]\n        dst_mean = [0.0, 0.0]\n        for i in range(0,SRC_NUM * 2,2):\n            src_mean[0] += src[i]\n            src_mean[1] += src[i + 1]\n            dst_mean[0] += self.umeyama_args_112[i]\n            dst_mean[1] += self.umeyama_args_112[i + 1]\n        src_mean[0] /= SRC_NUM\n        src_mean[1] /= SRC_NUM\n        dst_mean[0] /= SRC_NUM\n        dst_mean[1] /= SRC_NUM\n        src_demean = [[0.0, 0.0] for _ in range(SRC_NUM)]\n        dst_demean = [[0.0, 0.0] for _ in range(SRC_NUM)]\n        for i in range(SRC_NUM):\n            src_demean[i][0] = src[2 * i] - src_mean[0]\n            src_demean[i][1] = src[2 * i + 1] - src_mean[1]\n            dst_demean[i][0] = self.umeyama_args_112[2 * i] - dst_mean[0]\n            dst_demean[i][1] = self.umeyama_args_112[2 * i + 1] - dst_mean[1]\n        A = [[0.0, 0.0], [0.0, 0.0]]\n        for i in range(SRC_DIM):\n            for k in range(SRC_DIM):\n                for j in range(SRC_NUM):\n                    A[i][k] += dst_demean[j][i] * src_demean[j][k]\n                A[i][k] /= SRC_NUM\n        T = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n        U, S, V = self.svd22([A[0][0], A[0][1], A[1][0], A[1][1]])\n        T[0][0] = U[0] * V[0] + U[1] * V[2]\n        T[0][1] = U[0] * V[1] + U[1] * V[3]\n        T[1][0] = U[2] * V[0] + U[3] * V[2]\n        T[1][1] = U[2] * V[1] + U[3] * V[3]\n        scale = 1.0\n        src_demean_mean = [0.0, 0.0]\n        src_demean_var = [0.0, 0.0]\n        for i in range(SRC_NUM):\n            src_demean_mean[0] += src_demean[i][0]\n            src_demean_mean[1] += src_demean[i][1]\n        src_demean_mean[0] /= SRC_NUM\n        src_demean_mean[1] /= SRC_NUM\n        for i in range(SRC_NUM):\n            src_demean_var[0] += (src_demean_mean[0] - src_demean[i][0]) * (src_demean_mean[0] - src_demean[i][0])\n            src_demean_var[1] += (src_demean_mean[1] - src_demean[i][1]) * (src_demean_mean[1] - src_demean[i][1])\n        src_demean_var[0] /= SRC_NUM\n        src_demean_var[1] /= SRC_NUM\n        scale = 1.0 / (src_demean_var[0] + src_demean_var[1]) * (S[0] + S[1])\n        T[0][2] = dst_mean[0] - scale * (T[0][0] * src_mean[0] + T[0][1] * src_mean[1])\n        T[1][2] = dst_mean[1] - scale * (T[1][0] * src_mean[0] + T[1][1] * src_mean[1])\n        T[0][0] *= scale\n        T[0][1] *= scale\n        T[1][0] *= scale\n        T[1][1] *= scale\n        return T\n\n    def get_affine_matrix(self,sparse_points):\n        # \u83b7\u53d6affine\u53d8\u6362\u77e9\u9635\n        with ScopedTiming("get_affine_matrix", self.debug_mode > 1):\n            # \u4f7f\u7528Umeyama\u7b97\u6cd5\u8ba1\u7b97\u4eff\u5c04\u53d8\u6362\u77e9\u9635\n            matrix_dst = self.image_umeyama_112(sparse_points)\n            matrix_dst = [matrix_dst[0][0],matrix_dst[0][1],matrix_dst[0][2],\n                          matrix_dst[1][0],matrix_dst[1][1],matrix_dst[1][2]]\n            return matrix_dst\n\n# \u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u7c7b\nclass FaceRecognition:\n    def __init__(self,face_det_kmodel,face_reg_kmodel,det_input_size,reg_input_size,database_dir,anchors,confidence_threshold=0.25,nms_threshold=0.3,face_recognition_threshold=0.75,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u8138\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n        self.face_reg_kmodel=face_reg_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u8bc6\u522b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.reg_input_size=reg_input_size\n        self.database_dir=database_dir\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.face_recognition_threshold=face_recognition_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.max_register_face = 100                  # \u6570\u636e\u5e93\u6700\u591a\u4eba\u8138\u4e2a\u6570\n        self.feature_num = 128                        # \u4eba\u8138\u8bc6\u522b\u7279\u5f81\u7ef4\u5ea6\n        self.valid_register_face = 0                  # \u5df2\u6ce8\u518c\u4eba\u8138\u6570\n        self.db_name= []\n        self.db_data= []\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.face_reg=FaceRegistrationApp(self.face_reg_kmodel,model_input_size=self.reg_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.face_det.config_preprocess()\n        # \u4eba\u8138\u6570\u636e\u5e93\u521d\u59cb\u5316\n        self.database_init()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u4eba\u8138\u68c0\u6d4b\n        det_boxes,landms=self.face_det.run(input_np)\n        recg_res = []\n        for landm in landms:\n            # \u9488\u5bf9\u6bcf\u4e2a\u4eba\u8138\u4e94\u5b98\u70b9\uff0c\u63a8\u7406\u5f97\u5230\u4eba\u8138\u7279\u5f81\uff0c\u5e76\u8ba1\u7b97\u7279\u5f81\u5728\u6570\u636e\u5e93\u4e2d\u76f8\u4f3c\u5ea6\n            self.face_reg.config_preprocess(landm)\n            feature=self.face_reg.run(input_np)\n            res = self.database_search(feature)\n            recg_res.append(res)\n        return det_boxes,recg_res\n\n    def database_init(self):\n        # \u6570\u636e\u521d\u59cb\u5316\uff0c\u6784\u5efa\u6570\u636e\u5e93\u4eba\u540d\u5217\u8868\u548c\u6570\u636e\u5e93\u7279\u5f81\u5217\u8868\n        with ScopedTiming("database_init", self.debug_mode > 1):\n            db_file_list = os.listdir(self.database_dir)\n            for db_file in db_file_list:\n                if not db_file.endswith(\'.bin\'):\n                    continue\n                if self.valid_register_face >= self.max_register_face:\n                    break\n                valid_index = self.valid_register_face\n                full_db_file = self.database_dir + db_file\n                with open(full_db_file, \'rb\') as f:\n                    data = f.read()\n                feature = np.frombuffer(data, dtype=np.float)\n                self.db_data.append(feature)\n                name = db_file.split(\'.\')[0]\n                self.db_name.append(name)\n                self.valid_register_face += 1\n\n    def database_reset(self):\n        # \u6570\u636e\u5e93\u6e05\u7a7a\n        with ScopedTiming("database_reset", self.debug_mode > 1):\n            print("database clearing...")\n            self.db_name = []\n            self.db_data = []\n            self.valid_register_face = 0\n            print("database clear Done!")\n\n    def database_search(self,feature):\n        # \u6570\u636e\u5e93\u67e5\u8be2\n        with ScopedTiming("database_search", self.debug_mode > 1):\n            v_id = -1\n            v_score_max = 0.0\n            # \u5c06\u5f53\u524d\u4eba\u8138\u7279\u5f81\u5f52\u4e00\u5316\n            feature /= np.linalg.norm(feature)\n            # \u904d\u5386\u5f53\u524d\u4eba\u8138\u6570\u636e\u5e93\uff0c\u7edf\u8ba1\u6700\u9ad8\u5f97\u5206\n            for i in range(self.valid_register_face):\n                db_feature = self.db_data[i]\n                db_feature /= np.linalg.norm(db_feature)\n                # \u8ba1\u7b97\u6570\u636e\u5e93\u7279\u5f81\u4e0e\u5f53\u524d\u4eba\u8138\u7279\u5f81\u76f8\u4f3c\u5ea6\n                v_score = np.dot(feature, db_feature)/2 + 0.5\n                if v_score > v_score_max:\n                    v_score_max = v_score\n                    v_id = i\n            if v_id == -1:\n                # \u6570\u636e\u5e93\u4e2d\u65e0\u4eba\u8138\n                return \'unknown\'\n            elif v_score_max < self.face_recognition_threshold:\n                # \u5c0f\u4e8e\u4eba\u8138\u8bc6\u522b\u9608\u503c\uff0c\u672a\u8bc6\u522b\n                return \'unknown\'\n            else:\n                # \u8bc6\u522b\u6210\u529f\n                result = \'name: {}, score:{}\'.format(self.db_name[v_id],v_score_max)\n                return result\n\n    # \u7ed8\u5236\u8bc6\u522b\u7ed3\u679c\n    def draw_result(self,pl,dets,recg_results):\n        pl.osd_img.clear()\n        if dets:\n            for i,det in enumerate(dets):\n                # \uff081\uff09\u753b\u4eba\u8138\u6846\n                x1, y1, w, h = map(lambda x: int(round(x, 0)), det[:4])\n                x1 = x1 * self.display_size[0]//self.rgb888p_size[0]\n                y1 = y1 * self.display_size[1]//self.rgb888p_size[1]\n                w =  w * self.display_size[0]//self.rgb888p_size[0]\n                h = h * self.display_size[1]//self.rgb888p_size[1]\n                pl.osd_img.draw_rectangle(x1,y1, w, h, color=(255,0, 0, 255), thickness = 4)\n                # \uff082\uff09\u5199\u4eba\u8138\u8bc6\u522b\u7ed3\u679c\n                recg_text = recg_results[i]\n                pl.osd_img.draw_string_advanced(x1,y1,32,recg_text,color=(255, 255, 0, 0))\n\n\nif __name__=="__main__":\n    # \u6ce8\u610f\uff1a\u6267\u884c\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e4b\u524d\uff0c\u9700\u8981\u5148\u6267\u884c\u4eba\u8138\u6ce8\u518c\u4efb\u52a1\u8fdb\u884c\u4eba\u8138\u8eab\u4efd\u6ce8\u518c\u751f\u6210feature\u6570\u636e\u5e93\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"lcd"\n    display_mode="lcd"\n    display_size=[640,480]\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/examples/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n    face_reg_kmodel_path="/sdcard/examples/kmodel/face_recognition.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/examples/utils/prior_data_320.bin"\n    database_dir ="/sdcard/examples/utils/db/"\n    rgb888p_size=[1024,768]\n    face_det_input_size=[320,320]\n    face_reg_input_size=[112,112]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n    face_recognition_threshold = 0.75        # \u4eba\u8138\u8bc6\u522b\u9608\u503c\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    sensor = Sensor(width=1280, height=960) # \u6784\u5efa\u6444\u50cf\u5934\u5bf9\u8c61\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create(sensor=sensor)  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    fr=FaceRecognition(face_det_kmodel_path,face_reg_kmodel_path,det_input_size=face_det_input_size,reg_input_size=face_reg_input_size,database_dir=database_dir,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,face_recognition_threshold=face_recognition_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total", 1):\n                img=pl.get_frame()                      # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,recg_res=fr.run(img)          # \u63a8\u7406\u5f53\u524d\u5e27\n#                print(det_boxes,recg_res)               # \u6253\u5370\u7ed3\u679c\n                fr.draw_result(pl,det_boxes,recg_res)   # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                         # \u5c55\u793a\u63a8\u7406\u6548\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        fr.face_det.deinit()\n        fr.face_reg.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.p,{children:"\u53ef\u4ee5\u770b\u5230\u4e00\u5f00\u59cb\u662f\u5148\u5b9a\u4e49\u663e\u793a\u6a21\u5f0f\u3001\u56fe\u50cf\u5927\u5c0f\u3001\u6a21\u578b\u76f8\u5173\u7684\u4e00\u4e9b\u53d8\u91cf\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u63a5\u7740\u662f\u901a\u8fc7\u521d\u59cb\u5316PipeLine\uff0c\u8fd9\u91cc\u4e3b\u8981\u521d\u59cb\u5316sensor\u548cdisplay\u6a21\u5757\uff0c\u914d\u7f6e\u6444\u50cf\u5934\u8f93\u51fa\u4e24\u8def\u4e0d\u540c\u7684\u683c\u5f0f\u548c\u5927\u5c0f\u7684\u56fe\u50cf\uff0c\u4ee5\u53ca\u8bbe\u7f6e\u663e\u793a\u6a21\u5f0f\uff0c\u5b8c\u6210\u521b\u5efaPipeLine\u5b9e\u4f8b\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u7136\u540e\u8c03\u7528\u81ea\u5b9a\u4e49FaceRecognition\u7c7b\u6784\u5efa\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u7c7b\uff0cFaceRecognition\u7c7b\u4f1a\u901a\u8fc7\u8c03\u7528FaceDetApp\u7c7b\u548cFaceRegistrationApp\u7c7b\u5b8c\u6210\u5bf9AIBase\u63a5\u53e3\u7684\u521d\u59cb\u5316\u4ee5\u53ca\u4f7f\u7528Ai2D\u63a5\u53e3\u7684\u65b9\u6cd5\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u548c\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u8f93\u5165\u56fe\u50cf\u7684\u9884\u5904\u7406\u65b9\u6cd5\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u6700\u540e\u5728\u4e00\u4e2a\u5faa\u73af\u4e2d\u4e0d\u65ad\u5730\u83b7\u53d6\u6444\u50cf\u5934\u8f93\u51fa\u7684RGBP888\u683c\u5f0f\u7684\u56fe\u50cf\u5e27\uff0c\u7136\u540e\u4f9d\u6b21\u5c06\u56fe\u50cf\u8f93\u5165\u5230\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u3001\u4eba\u8138\u8bc6\u522b\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u518d\u5c06\u63d0\u53d6\u7684\u7279\u5f81\u503c\u4e0e\u6ce8\u518c\u4eba\u8138\u6570\u636e\u5e93\u7684\u6570\u636e\u8fdb\u884c\u6bd4\u5bf9\uff0c\u7136\u540e\u5c06\u8bc6\u522b\u7ed3\u679c\u901a\u8fc7print\u6253\u5370\uff0c\u540c\u65f6\u5c06\u4eba\u8138\u7684\u8eab\u4efd\u4fe1\u606f\u7ed8\u5236\u5230\u56fe\u50cf\u4e0a\uff0c\u5e76\u5728LCD\u4e0a\u663e\u793a\u56fe\u50cf\u3002"}),"\n",(0,i.jsx)(n.h2,{id:"\u8fd0\u884c\u9a8c\u8bc1",children:"\u8fd0\u884c\u9a8c\u8bc1"}),"\n",(0,i.jsx)(n.h3,{id:"\u4eba\u8138\u4fe1\u606f\u6ce8\u518c",children:"\u4eba\u8138\u4fe1\u606f\u6ce8\u518c"}),"\n",(0,i.jsxs)(n.p,{children:["\u5148\u8fd0\u884c\u4eba\u8138\u6ce8\u518c\u7a0b\u5e8f\uff0c\u4eba\u8138\u6ce8\u518c\u7a0b\u5e8f\u4f1a\u5bf9CanMV\u76d8",(0,i.jsx)(n.code,{children:"\\sdcard\\examples\\utils\\db_img\\"}),"\u76ee\u5f55\u4e0b\u7684\u4eba\u8138\u56fe\u7247\u8fdb\u884c\u7279\u5f81\u503c\u63d0\u53d6\uff0c\u53ef\u4ee5\u770b\u5230\u6b64\u8def\u5f84\u4e0b\u7cfb\u7edf\u9ed8\u8ba4\u9884\u5b58\u4e86\u4e24\u5f20\u4eba\u8138\u56fe\u7247\uff0c\u7528\u6237\u4e5f\u53ef\u4ee5\u628a\u81ea\u5df1\u60f3\u8bc6\u522b\u7684\u4eba\u8138\u56fe\u7247\u653e\u8fdb\u53bb\u3002"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"01",src:s(4823).A+"",width:"246",height:"163"})}),"\n",(0,i.jsxs)(n.p,{children:["\u6211\u4eec\u5728CanMV IDE\u70b9\u51fb\u8fd0\u884cmain1.py\u7a0b\u5e8f\u540e\uff0c\u53ef\u4ee5\u5728",(0,i.jsx)(n.code,{children:"\\sdcard\\examples\\utils\\db\\"}),"\u76ee\u5f55\u4e0b\u770b\u5230\u4e24\u4e2a\u65b0\u751f\u6210\u7684BIN\u6587\u4ef6\uff0c\u8fd9\u4e24\u4e2aBIN"]}),"\n",(0,i.jsx)(n.p,{children:"\u6587\u4ef6\u50a8\u5b58\u7740\u521a\u521a\u4e24\u4e2a\u4eba\u8138\u4fe1\u606f\uff0c\u53ef\u4ee5\u6839\u636e\u539f\u56fe\u50cf\u7684\u6587\u4ef6\u540d\u79f0\u8fdb\u884c\u533a\u5206\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"01",src:s(5326).A+"",width:"517",height:"188"})}),"\n",(0,i.jsxs)(n.p,{children:["\u81f3\u6b64\uff0c\u4eba\u8138\u6ce8\u518c\u5b8c\u6210\u3002\u63a5\u4e0b\u6765\u8fd0\u884c",(0,i.jsx)(n.strong,{children:"\u4eba\u8138\u8bc6\u522b\u7a0b\u5e8f"})]}),"\n",(0,i.jsx)(n.h3,{id:"\u4eba\u8138\u8bc6\u522b",children:"\u4eba\u8138\u8bc6\u522b"}),"\n",(0,i.jsx)(n.p,{children:"\u4e0b\u9762\u6211\u4eec\u7528\u4e24\u5f20\u521a\u521a\u5f55\u5165\u7684\u4eba\u8138\u548c\u4e00\u5f20\u672a\u5f55\u5165\u7684\u4eba\u8138\u56fe\u7247\u8fdb\u884c\u6d4b\u8bd5\uff0c\u987a\u5e8f\u5982\u4e0b\u56fe\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"01",src:s(24708).A+"",width:"868",height:"296"})}),"\n",(0,i.jsx)(n.p,{children:"\u5c06K230D BOX\u5f00\u53d1\u677f\u8fde\u63a5CanMV IDE\uff0c\u70b9\u51fbCanMV IDE\u4e0a\u7684\u201c\u5f00\u59cb(\u8fd0\u884c\u811a\u672c)\u201d\u6309\u94ae\u540e\uff0c\u5c06\u6444\u50cf\u5934\u5bf9\u51c6\u9700\u8981\u8bc6\u522b\u7684\u4eba\u8138\uff0c\u8ba9\u5176\u91c7\u96c6\u5230\u4eba\u8138\u56fe\u50cf\uff0c\u968f\u540e\u4fbf\u80fd\u5728LCD\u4e0a\u770b\u5230\u6444\u50cf\u5934\u8f93\u51fa\u7684\u56fe\u50cf\uff0c\u4ee5\u53ca\u7ecf\u8fc7\u4eba\u8138\u8bc6\u522b\u540e\u7684\u4eba\u8138\u8eab\u4efd\u4fe1\u606f\uff0c\u6211\u4eec\u4f9d\u6b21\u5bf9\u4e0a\u56fe\u4eba\u8138\u8fdb\u884c\u8bc6\u522b\uff0c\u9996\u5148\u662fID1\u7684\u4eba\u8138\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"01",src:s(81005).A+"",width:"501",height:"387"})}),"\n",(0,i.jsx)(n.p,{children:"\u8bc6\u522bID2\u7684\u4eba\u8138\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"01",src:s(99382).A+"",width:"449",height:"386"})}),"\n",(0,i.jsx)(n.p,{children:"\u5f53\u6211\u4eec\u4f7f\u7528\u672a\u6ce8\u518c\u7684\u4eba\u8138\uff0c\u4f1a\u63d0\u793aunknown\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"01",src:s(79359).A+"",width:"464",height:"391"})})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(o,{...e})}):o(e)}},79359:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/13-4eeb4a2849a508214c22ee0257432168.png"},81005:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/11-8a21b2aaf23f32c9c677fffd0e9c0bee.png"},99382:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/12-84f5ecd16f45c5112a703822a8360c43.png"}}]);