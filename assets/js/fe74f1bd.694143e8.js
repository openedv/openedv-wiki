"use strict";(self.webpackChunkopenedv_wiki=self.webpackChunkopenedv_wiki||[]).push([[77640],{5113:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/65-932cf852957cd06f7910cd322662b379.png"},28453:(e,n,s)=>{s.d(n,{R:()=>_,x:()=>r});var i=s(96540);const l={},t=i.createContext(l);function _(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:_(e.components),i.createElement(t.Provider,{value:n},e.children)}},77342:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>_,default:()=>o,frontMatter:()=>t,metadata:()=>r,toc:()=>d});var i=s(74848),l=s(28453);const t={title:"\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5b9e\u9a8c",sidebar_position:12},_="\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5b9e\u9a8c",r={id:"Boards/Kendryte/DNK230D/example-ai/dynamic_gesture",title:"\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5b9e\u9a8c",description:"\u524d\u8a00",source:"@site/docs/Boards/Kendryte/DNK230D/example-ai/dynamic_gesture.md",sourceDirName:"Boards/Kendryte/DNK230D/example-ai",slug:"/Boards/Kendryte/DNK230D/example-ai/dynamic_gesture",permalink:"/docs/Boards/Kendryte/DNK230D/example-ai/dynamic_gesture",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:12,frontMatter:{title:"\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5b9e\u9a8c",sidebar_position:12},sidebar:"BoardsKendryteDNK230DSidebar",previous:{title:"\u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b\u5b9e\u9a8c",permalink:"/docs/Boards/Kendryte/DNK230D/example-ai/hand_keypoint_class"},next:{title:"\u5c40\u90e8\u653e\u5927\u5668\u5b9e\u9a8c",permalink:"/docs/Boards/Kendryte/DNK230D/example-ai/space_resize"}},a={},d=[{value:"\u524d\u8a00",id:"\u524d\u8a00",level:2},{value:"AI\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd",id:"ai\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd",level:2},{value:"\u786c\u4ef6\u8bbe\u8ba1",id:"\u786c\u4ef6\u8bbe\u8ba1",level:2},{value:"\u4f8b\u7a0b\u529f\u80fd",id:"\u4f8b\u7a0b\u529f\u80fd",level:3},{value:"\u786c\u4ef6\u8d44\u6e90",id:"\u786c\u4ef6\u8d44\u6e90",level:3},{value:"\u539f\u7406\u56fe",id:"\u539f\u7406\u56fe",level:3},{value:"\u5b9e\u9a8c\u4ee3\u7801",id:"\u5b9e\u9a8c\u4ee3\u7801",level:2},{value:"\u8fd0\u884c\u9a8c\u8bc1",id:"\u8fd0\u884c\u9a8c\u8bc1",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",...(0,l.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5b9e\u9a8c",children:"\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5b9e\u9a8c"})}),"\n",(0,i.jsx)(n.h2,{id:"\u524d\u8a00",children:"\u524d\u8a00"}),"\n",(0,i.jsx)(n.p,{children:"\u5728\u4e0a\u4e00\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u5b66\u4e60\u4e86\u5982\u4f55\u5728CanMV\u4e0b\u4f7f\u7528CanMV AI\u89c6\u89c9\u5f00\u53d1\u6846\u67b6\u548cMicroPython\u7f16\u7a0b\u65b9\u6cd5\u5b9e\u73b0\u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b\u7684\u529f\u80fd\uff0c\u672c\u7ae0\u5c06\u901a\u8fc7\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5b9e\u9a8c\uff0c\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528CanMV AI\u89c6\u89c9\u5f00\u53d1\u6846\u67b6\u548cMicroPython\u7f16\u7a0b\u5b9e\u73b0\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u7684\u529f\u80fd\u3002\u672c\u5b9e\u9a8c\u7531\u4e0a\u4e2a\u5b9e\u9a8c\u6269\u5c55\u800c\u6765\uff0c\u989d\u5916\u589e\u52a0\u4e00\u4e2a\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\uff0c\u4e0e\u4e0a\u4e2a\u5b9e\u9a8c\u7528\u5230\u7684\u4e24\u4e2a\u6a21\u578b\u914d\u5408\u5e94\u7528\uff0c\u4ee3\u7801\u6bd4\u8f83\u590d\u6742\uff0c\u6211\u4eec\u9996\u5148\u91c7\u96c6\u6444\u50cf\u5934\u6355\u83b7\u7684\u56fe\u50cf\uff0c\u7136\u540e\u7ecf\u8fc7\u56fe\u50cf\u9884\u5904\u7406\u3001\u6a21\u578b\u63a8\u7406\u548c\u8f93\u51fa\u5904\u7406\u7ed3\u679c\u7b49\u4e00\u7cfb\u5217\u6b65\u9aa4\uff0c\u5b8c\u6210\u624b\u638c\u68c0\u6d4b\u7684\u529f\u80fd\uff0c\u7136\u540e\u5728\u68c0\u6d4b\u5230\u624b\u638c\u7684\u533a\u57df\uff0c\u8fdb\u4e00\u6b65\u4f7f\u7528\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u4ece\u800c\u83b7\u5f97\u5177\u4f53\u624b\u52bf\uff0c\u7136\u540e\u518d\u4f7f\u7528\u52a8\u6001\u624b\u52bf\u6a21\u578b\u8fdb\u884c\u52a8\u6001\u8bc6\u522b\uff0c\u901a\u8fc7\u5bf9\u624b\u52bf\u7684\u9759\u6001\u8bc6\u522b\u548c\u624b\u7684\u52a8\u6001\u68c0\u6d4b\uff0c\u4ece\u800c\u5b9e\u73b0\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u7684\u529f\u80fd\u3002\u901a\u8fc7\u672c\u7ae0\u7684\u5b66\u4e60\uff0c\u8bfb\u8005\u5c06\u638c\u63e1\u5982\u4f55\u5728CanMV\u4e0b\u4f7f\u7528CanMV AI\u89c6\u89c9\u5f00\u53d1\u6846\u67b6\u548cMicroPython\u7f16\u7a0b\u65b9\u6cd5\u5b9e\u73b0\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u7684\u65b9\u6cd5\u3002"}),"\n",(0,i.jsx)(n.h2,{id:"ai\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd",children:"AI\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd"}),"\n",(0,i.jsxs)(n.p,{children:["\u4e3a\u4e86\u7b80\u5316AI\u5f00\u53d1\u6d41\u7a0b\u5e76\u964d\u4f4eAI\u5f00\u53d1\u96be\u5ea6\uff0cCanMV\u5b98\u65b9\u9488\u5bf9K230D\u4e13\u95e8\u642d\u5efa\u4e86AI\u5f00\u53d1\u6846\u67b6\uff0c\u6709\u5173AI\u5f00\u53d1\u6846\u67b6\u7684\u4ecb\u7ecd\uff0c\u8bf7\u89c1",(0,i.jsx)(n.a,{href:"/docs/Boards/Kendryte/DNK230D/example-ai/development_framework",children:"CanMV AI\u5f00\u53d1\u6846\u67b6"})]}),"\n",(0,i.jsx)(n.h2,{id:"\u786c\u4ef6\u8bbe\u8ba1",children:"\u786c\u4ef6\u8bbe\u8ba1"}),"\n",(0,i.jsx)(n.h3,{id:"\u4f8b\u7a0b\u529f\u80fd",children:"\u4f8b\u7a0b\u529f\u80fd"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"\u83b7\u53d6\u6444\u50cf\u5934\u8f93\u51fa\u7684\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u56fe\u50cf\u8f93\u5165\u5230CanMV K230D\u7684AI\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002\u672c\u5b9e\u9a8c\u4f7f\u7528\u4e86\u4e09\u4e2aAI\u6a21\u578b\uff1a\u4e24\u4e2a\u662f\u4e0a\u4e2a\u5b9e\u9a8c\u4f7f\u7528\u7684\u624b\u638c\u68c0\u6d4b\u6a21\u578b\u548c\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u578b\uff0c\u989d\u5916\u589e\u52a0\u4e86\u4e00\u4e2a\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u3002\u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8d1f\u8d23\u627e\u51fa\u56fe\u50cf\u4e2d\u7684\u624b\u638c\u533a\u57df\uff0c\u7136\u540e\u5c06\u8be5\u533a\u57df\u4f20\u9012\u7ed9\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u624b\u638c\u5173\u952e\u70b9\u4f4d\u7f6e\u7684\u63a8\u7406\u3002\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u578b\u80fd\u5c06\u8f93\u5165\u6a21\u578b\u7684\u624b\u638c\u56fe\u8fdb\u884c\u68c0\u6d4b\uff0c\u5b9e\u73b0\u5bf9\u624b\u52bf\u7684\u5224\u65ad\uff0c\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u4e3b\u8981\u5bf9\u624b\u52bf\u7684\u52a8\u6001\u68c0\u6d4b\uff0c\u901a\u8fc7\u5bf9\u624b\u7684\u9759\u6001\u68c0\u6d4b\u548c\u624b\u7684\u52a8\u6001\u8bc6\u522b\uff0c\u5b9e\u73b0\u4e86\u4e94\u79cd\u52a8\u6001\u624b\u52bf\u7684\u8bc6\u522b\uff0c\u4e94\u79cd\u624b\u52bf\u5305\u62ec\uff1a\u4e0a\u6325\u624b\u3001\u4e0b\u6325\u624b\u3001\u5de6\u6325\u624b\u3001\u53f3\u6325\u624b\u3001\u624b\u6307\u634f\u5408\u4e94\u4e2a\u624b\u52bf\u3002\u6700\u540e\uff0c\u5c06\u5904\u7406\u540e\u7684\u56fe\u50cf\u663e\u793a\u5728LCD\u4e0a\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"\u786c\u4ef6\u8d44\u6e90",children:"\u786c\u4ef6\u8d44\u6e90"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"\u672c\u7ae0\u5b9e\u9a8c\u5185\u5bb9\u4e3b\u8981\u8bb2\u89e3K230D\u7684\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668KPU\u7684\u4f7f\u7528\uff0c\u65e0\u9700\u5173\u6ce8\u786c\u4ef6\u8d44\u6e90\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"\u539f\u7406\u56fe",children:"\u539f\u7406\u56fe"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"\u672c\u7ae0\u5b9e\u9a8c\u5185\u5bb9\u4e3b\u8981\u8bb2\u89e3K230D\u7684\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668KPU\u7684\u4f7f\u7528\uff0c\u65e0\u9700\u5173\u6ce8\u539f\u7406\u56fe\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"\u5b9e\u9a8c\u4ee3\u7801",children:"\u5b9e\u9a8c\u4ee3\u7801"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nfrom random import randint\nimport os\nimport ujson\nfrom media.media import *\nfrom media.sensor import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandDetApp(AIBase):\n    def __init__(self,kmodel_path,labels,model_input_size,anchors,confidence_threshold=0.2,nms_threshold=0.5,nms_option=False, strides=[8,16,32],rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6807\u7b7e\n        self.labels=labels\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # \u68c0\u6d4b\u951a\u6846\n        self.anchors=anchors\n        self.strides = strides  # \u7279\u5f81\u4e0b\u91c7\u6837\u500d\u6570\n        self.nms_option = nms_option  # NMS\u9009\u9879\uff0c\u5982\u679c\u4e3aTrue\u505a\u7c7b\u95f4NMS,\u5982\u679c\u4e3aFalse\u505a\u7c7b\u5185NMS\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86padding\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [114, 114, 114])\n            # \u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u8fdb\u884cresize\u64cd\u4f5c\uff0c\u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u4ee5\u7b26\u5408\u6a21\u578b\u8f93\u5165\u8981\u6c42\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\u8fc7\u7a0b\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u7684anchorbasedet_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            # \u8fd4\u56de\u624b\u638c\u68c0\u6d4b\u7ed3\u679c\n            return dets\n\n    # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n    def get_padding_param(self):\n        # \u6839\u636e\u76ee\u6807\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8ba1\u7b97\u6bd4\u4f8b\u56e0\u5b50\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        # \u9009\u62e9\u8f83\u5c0f\u7684\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf\u5185\u5bb9\u5b8c\u6574\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        # \u8ba1\u7b97\u65b0\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        # \u8ba1\u7b97\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u7684\u5dee\u503c\uff0c\u5e76\u786e\u5b9apadding\u7684\u4f4d\u7f6e\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw + 0.1))\n        return top, bottom, left, right\n\n# \u81ea\u5b9a\u4e49\u624b\u52bf\u5173\u952e\u70b9\u5206\u7c7b\u4efb\u52a1\u7c7b\nclass HandKPClassApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # crop\u53c2\u6570\u5217\u8868\n        self.crop_params=[]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u5982\u679cinput_image_size\u4e3aNone,\u4f7f\u7528\u89c6\u9891\u51fa\u56fe\u5927\u5c0f\uff0c\u5426\u5219\u6309\u7167\u81ea\u5b9a\u4e49\u8bbe\u7f6e\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97crop\u53c2\u6570\n            self.crop_params = self.get_crop_param(det)\n            # \u8bbe\u7f6ecrop\u9884\u5904\u7406\u8fc7\u7a0b\n            self.ai2d.crop(self.crop_params[0],self.crop_params[1],self.crop_params[2],self.crop_params[3])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\u8fc7\u7a0b\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # build\u9884\u5904\u7406\u8fc7\u7a0b\uff0c\u53c2\u6570\u4e3a\u8f93\u5165tensor\u7684shape\u548c\u8f93\u51fatensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            results=results[0].reshape(results[0].shape[0]*results[0].shape[1])\n            results_show = np.zeros(results.shape,dtype=np.int16)\n            results_show[0::2] = results[0::2] * self.crop_params[3] + self.crop_params[0]\n            results_show[1::2] = results[1::2] * self.crop_params[2] + self.crop_params[1]\n            # \u6839\u636e\u8f93\u51fa\u8ba1\u7b97\u624b\u52bf\n            gesture=self.hk_gesture(results_show)\n            return results_show,gesture\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self,det_box):\n        x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n        w,h= int(x2 - x1),int(y2 - y1)\n        w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n        h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n        x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n        y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n        length = max(w, h)/2\n        cx = (x1+x2)/2\n        cy = (y1+y2)/2\n        ratio_num = 1.26*length\n        x1_kp = int(max(0,cx-ratio_num))\n        y1_kp = int(max(0,cy-ratio_num))\n        x2_kp = int(min(self.rgb888p_size[0]-1, cx+ratio_num))\n        y2_kp = int(min(self.rgb888p_size[1]-1, cy+ratio_num))\n        w_kp = int(x2_kp - x1_kp + 1)\n        h_kp = int(y2_kp - y1_kp + 1)\n        return [x1_kp, y1_kp, w_kp, h_kp]\n\n    # \u6c42\u4e24\u4e2avector\u4e4b\u95f4\u7684\u5939\u89d2\n    def hk_vector_2d_angle(self,v1,v2):\n        with ScopedTiming("hk_vector_2d_angle",self.debug_mode > 0):\n            v1_x,v1_y,v2_x,v2_y = v1[0],v1[1],v2[0],v2[1]\n            v1_norm = np.sqrt(v1_x * v1_x+ v1_y * v1_y)\n            v2_norm = np.sqrt(v2_x * v2_x + v2_y * v2_y)\n            dot_product = v1_x * v2_x + v1_y * v2_y\n            cos_angle = dot_product/(v1_norm*v2_norm)\n            angle = np.acos(cos_angle)*180/np.pi\n            return angle\n\n    # \u6839\u636e\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u7ed3\u679c\u5224\u65ad\u624b\u52bf\u7c7b\u522b\n    def hk_gesture(self,results):\n        with ScopedTiming("hk_gesture",self.debug_mode > 0):\n            angle_list = []\n            for i in range(5):\n                angle = self.hk_vector_2d_angle([(results[0]-results[i*8+4]), (results[1]-results[i*8+5])],[(results[i*8+6]-results[i*8+8]),(results[i*8+7]-results[i*8+9])])\n                angle_list.append(angle)\n            thr_angle,thr_angle_thumb,thr_angle_s,gesture_str = 65.,53.,49.,None\n            if 65535. not in angle_list:\n                if (angle_list[0]>thr_angle_thumb)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "fist"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]<thr_angle_s) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "five"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "gun"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "love"\n                elif (angle_list[0]>5)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "one"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "six"\n                elif (angle_list[0]>thr_angle_thumb)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]<thr_angle_s) and (angle_list[4]>thr_angle):\n                    gesture_str = "three"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "thumbUp"\n                elif (angle_list[0]>thr_angle_thumb)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "yeah"\n            return gesture_str\n\n# \u81ea\u5b9a\u4e49\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\u7c7b\nclass DynamicGestureApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u6ce8\u610f\uff1aai2d\u8bbe\u7f6e\u591a\u4e2a\u9884\u5904\u7406\u65f6\u6267\u884c\u7684\u987a\u5e8f\u4e3a\uff1acrop->shift->resize/affine->pad\uff0c\u5982\u679c\u4e0d\u7b26\u5408\u8be5\u987a\u5e8f\uff0c\u9700\u8981\u914d\u7f6e\u591a\u4e2aai2d\u5bf9\u8c61;\n        # \u5982\u4e0b\u6a21\u578b\u9884\u5904\u7406\u8981\u5148\u505aresize\u518d\u505acrop\uff0c\u56e0\u6b64\u8981\u914d\u7f6e\u4e24\u4e2aAi2d\u5bf9\u8c61\n        self.ai2d_resize=Ai2d(debug_mode)\n        self.ai2d_resize.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n        self.ai2d_crop=Ai2d(debug_mode)\n        self.ai2d_crop.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n        # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u8f93\u5165tensors\u5217\u8868\n        self.input_tensors=[]\n        # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u7684\u8f93\u5165tensor\u7684shape\n        self.gesture_kmodel_input_shape = [[1, 3, 224, 224],                     # \u52a8\u6001\u624b\u52bf\u8bc6\u522bkmodel\u8f93\u5165\u5206\u8fa8\u7387\n                                           [1,3,56,56],\n                                           [1,4,28,28],\n                                           [1,4,28,28],\n                                           [1,8,14,14],\n                                           [1,8,14,14],\n                                           [1,8,14,14],\n                                           [1,12,14,14],\n                                           [1,12,14,14],\n                                           [1,20,7,7],\n                                           [1,20,7,7]]\n        # \u9884\u5904\u7406\u53c2\u6570\n        self.resize_shape = 256\n        self.mean_values = np.array([0.485, 0.456, 0.406]).reshape((3,1,1))      # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u9884\u5904\u7406\u5747\u503c\n        self.std_values = np.array([0.229, 0.224, 0.225]).reshape((3,1,1))       # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u9884\u5904\u7406\u65b9\u5dee\n        self.first_data=None\n        self.max_hist_len=20\n        self.crop_params=self.get_crop_param()\n\n    # \u914d\u7f6e\u9884\u5904\u7406\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u914d\u7f6eresize\u548ccrop\u9884\u5904\u7406\n            self.ai2d_resize.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d_resize.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.crop_params[1],self.crop_params[0]])\n            self.ai2d_crop.crop(self.crop_params[2],self.crop_params[3],self.crop_params[4],self.crop_params[5])\n            self.ai2d_crop.build([1,3,self.crop_params[1],self.crop_params[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n            # \u521d\u59cb\u5316\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u8f93\u5165\u5217\u8868\n            inputs_num=self.get_kmodel_inputs_num()\n            self.first_data = np.ones(self.gesture_kmodel_input_shape[0], dtype=np.float)\n            for i in range(inputs_num):\n                data = np.zeros(self.gesture_kmodel_input_shape[i], dtype=np.float)\n                self.input_tensors.append(nn.from_numpy(data))\n\n    # \u91cd\u5199\u9884\u5904\u7406\uff0c\u56e0\u4e3a\u8be5\u90e8\u5206\u4e0d\u662f\u5355\u7eaf\u7684\u8d70\u4e00\u4e2aai2d\u505a\u9884\u5904\u7406\uff0c\u6240\u4ee5\u8be5\u51fd\u6570\u9700\u8981\u91cd\u5199\n    def preprocess(self,input_np):\n        # \u5148\u8d70resize\uff0c\u518d\u8d70crop\n        resize_tensor=self.ai2d_resize.run(input_np)\n        crop_output_tensor=self.ai2d_crop.run(resize_tensor.to_numpy())\n        ai2d_output = crop_output_tensor.to_numpy()\n        self.first_data[0] = ai2d_output[0].copy()\n        self.first_data[0] = (self.first_data[0]*1.0/255 -self.mean_values)/self.std_values\n        self.input_tensors[0]=nn.from_numpy(self.first_data)\n        return\n\n    # run\u51fd\u6570\u91cd\u5199\n    def run(self,input_np,his_logit,history):\n        # \u9884\u5904\u7406\n        self.preprocess(input_np)\n        # \u63a8\u7406\n        outputs=self.inference(self.input_tensors)\n        # \u4f7f\u7528\u5f53\u524d\u5e27\u7684\u8f93\u51fa\u66f4\u65b0\u4e0b\u4e00\u5e27\u7684\u8f93\u5165\u5217\u8868\n        outputs_num=self.get_kmodel_outputs_num()\n        for i in range(1,outputs_num):\n            self.input_tensors[i]=nn.from_numpy(outputs[i])\n        # \u8fd4\u56de\u540e\u5904\u7406\u7ed3\u679c\n        return self.postprocess(outputs,his_logit,history)\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\n    def postprocess(self,results,his_logit, history):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            his_logit.append(results[0])\n            avg_logit = sum(np.array(his_logit))\n            idx_ = np.argmax(avg_logit)\n            idx = self.gesture_process_output(idx_, history)\n            if (idx_ != idx):\n                his_logit_last = his_logit[-1]\n                his_logit = []\n                his_logit.append(his_logit_last)\n            return idx, avg_logit\n\n    # \u624b\u52bf\u5904\u7406\u51fd\u6570\n    def gesture_process_output(self,pred,history):\n        if (pred == 7 or pred == 8 or pred == 21 or pred == 22 or pred == 3 ):\n            pred = history[-1]\n        if (pred == 0 or pred == 4 or pred == 6 or pred == 9 or pred == 14 or pred == 1 or pred == 19 or pred == 20 or pred == 23 or pred == 24) :\n            pred = history[-1]\n        if (pred == 0) :\n            pred = 2\n        if (pred != history[-1]) :\n            if (len(history)>= 2) :\n                if (history[-1] != history[len(history)-2]) :\n                    pred = history[-1]\n        history.append(pred)\n        if (len(history) > self.max_hist_len) :\n            history = history[-self.max_hist_len:]\n        return history[-1]\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self):\n        ori_w = self.rgb888p_size[0]\n        ori_h = self.rgb888p_size[1]\n        width = self.model_input_size[0]\n        height = self.model_input_size[1]\n        ratiow = float(self.resize_shape) / ori_w\n        ratioh = float(self.resize_shape) / ori_h\n        if ratiow < ratioh:\n            ratio = ratioh\n        else:\n            ratio = ratiow\n        new_w = int(ratio * ori_w)\n        new_h = int(ratio * ori_h)\n        top = int((new_h-height)/2)\n        left = int((new_w-width)/2)\n        return new_w,new_h,left,top,width,height\n\n    # \u91cd\u5199\u9006\u521d\u59cb\u5316\n    def deinit(self):\n        with ScopedTiming("deinit",self.debug_mode > 0):\n            del self.kpu\n            del self.ai2d_resize\n            del self.ai2d_crop\n            self.tensors.clear()\n            del self.tensors\n            gc.collect()\n            nn.shrink_memory_pool()\n            os.exitpoint(os.EXITPOINT_ENABLE_SLEEP)\n            time.sleep_ms(100)\n\n# \u81ea\u5b9a\u4e49\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\nclass DynamicGesture:\n    def __init__(self,hand_det_kmodel,hand_kp_kmodel,gesture_kmodel,det_input_size,kp_input_size,gesture_input_size,labels,anchors,confidence_threshold=0.25,nms_threshold=0.3,nms_option=False,strides=[8,16,32],rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.hand_det_kmodel=hand_det_kmodel\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n        self.hand_kp_kmodel=hand_kp_kmodel\n        # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u8def\u5f84\n        self.gesture_kmodel=gesture_kmodel\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.kp_input_size=kp_input_size\n        # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.gesture_input_size=gesture_input_size\n        self.labels=labels\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.nms_option=nms_option\n        self.strides=strides\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u8d34\u56fe\n        self.bin_width = 150                                                     # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5c4f\u5e55\u5750\u4e0a\u89d2\u6807\u5fd7\u72b6\u6001\u6587\u4ef6\u7684\u77ed\u8fb9\u5c3a\u5bf8\n        self.bin_height = 216                                                    # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5c4f\u5e55\u5750\u4e0a\u89d2\u6807\u5fd7\u72b6\u6001\u6587\u4ef6\u7684\u957f\u8fb9\u5c3a\u5bf8\n        shang_argb = np.fromfile("/sdcard/examples/utils/shang.bin", dtype=np.uint8)\n        self.shang_argb = shang_argb.reshape((self.bin_height, self.bin_width, 4))\n        xia_argb = np.fromfile("/sdcard/examples/utils/xia.bin", dtype=np.uint8)\n        self.xia_argb = xia_argb.reshape((self.bin_height, self.bin_width, 4))\n        zuo_argb = np.fromfile("/sdcard/examples/utils/zuo.bin", dtype=np.uint8)\n        self.zuo_argb = zuo_argb.reshape((self.bin_width, self.bin_height, 4))\n        you_argb = np.fromfile("/sdcard/examples/utils/you.bin", dtype=np.uint8)\n        self.you_argb = you_argb.reshape((self.bin_width, self.bin_height, 4))\n        #\u5176\u4ed6\u53c2\u6570\n        self.TRIGGER = 0                                                         # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5e94\u7528\u7684\u7ed3\u679c\u72b6\u6001\n        self.MIDDLE = 1\n        self.UP = 2\n        self.DOWN = 3\n        self.LEFT = 4\n        self.RIGHT = 5\n        self.max_hist_len = 20                                                   # \u6700\u591a\u5b58\u50a8\u591a\u5c11\u5e27\u7684\u7ed3\u679c\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.cur_state = self.TRIGGER\n        self.pre_state = self.TRIGGER\n        self.draw_state = self.TRIGGER\n        self.vec_flag = []\n        self.his_logit = []\n        self.history = [2]\n        self.s_start = time.time_ns()\n        self.m_start=None\n        self.hand_det=HandDetApp(self.hand_det_kmodel,self.labels,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,nms_option=self.nms_option,strides=self.strides,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.hand_kp=HandKPClassApp(self.hand_kp_kmodel,model_input_size=self.kp_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.dg=DynamicGestureApp(self.gesture_kmodel,model_input_size=self.gesture_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.hand_det.config_preprocess()\n        self.dg.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        if self.cur_state == self.TRIGGER:\n            # \u624b\u638c\u68c0\u6d4b\n            det_boxes=self.hand_det.run(input_np)\n            boxes=[]\n            gesture_res=[]\n            for det_box in det_boxes:\n                # \u7b5b\u9009\u68c0\u6d4b\u6846\n                x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n                w,h= int(x2 - x1),int(y2 - y1)\n                if (h<(0.1*self.rgb888p_size[1])):\n                    continue\n                if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                    continue\n                if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                    continue\n                # \u624b\u638c\u5173\u952e\u70b9\u9884\u5904\u7406\u914d\u7f6e\n                self.hand_kp.config_preprocess(det_box)\n                # \u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\n                hk_results,gesture_str=self.hand_kp.run(input_np)\n                boxes.append(det_box)\n                gesture_res.append((hk_results,gesture_str))\n            return boxes,gesture_res\n        else:\n            # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\n            idx, avg_logit = self.dg.run(input_np, self.his_logit, self.history)\n            return idx,avg_logit\n\n    # \u6839\u636e\u8f93\u51fa\u7ed3\u679c\u7ed8\u5236\u6548\u679c\n    def draw_result(self,pl,output1,output2):\n        pl.osd_img.clear()\n        draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n        draw_img=image.Image(self.display_size[0], self.display_size[1], image.ARGB8888,alloc=image.ALLOC_REF,data=draw_img_np)\n        if self.cur_state == self.TRIGGER:\n            for i in range(len(output1)):\n                hk_results,gesture=output2[i][0],output2[i][1]\n                if ((gesture == "five") or (gesture == "yeah")):\n                    v_x = hk_results[24]-hk_results[0]\n                    v_y = hk_results[25]-hk_results[1]\n                    angle = self.hand_kp.hk_vector_2d_angle([v_x,v_y],[1.0,0.0])\n                    if (v_y>0):\n                        angle = 360-angle\n                    if ((70.0<=angle) and (angle<110.0)):                                                   # \u624b\u6307\u671d\u4e0a\n                        if ((self.pre_state != self.UP) or (self.pre_state != self.MIDDLE)):\n                            self.vec_flag.append(self.pre_state)\n                        if ((len(self.vec_flag)>10)or(self.pre_state == self.UP) or (self.pre_state == self.MIDDLE) or(self.pre_state == self.TRIGGER)):\n                            draw_img_np[:self.bin_height,:self.bin_width,:] = self.shang_argb\n                            self.cur_state = self.UP\n                    elif ((110.0<=angle) and (angle<225.0)):                                                # \u624b\u6307\u5411\u53f3(\u5b9e\u9645\u65b9\u5411)\n                        if (self.pre_state != self.RIGHT):\n                            self.vec_flag.append(self.pre_state)\n                        if ((len(self.vec_flag)>10)or(self.pre_state == self.RIGHT)or(self.pre_state == self.TRIGGER)):\n                            draw_img_np[:self.bin_width,:self.bin_height,:] = self.you_argb\n                            self.cur_state = self.RIGHT\n                    elif((225.0<=angle) and (angle<315.0)):                                                 # \u624b\u6307\u5411\u4e0b\n                        if (self.pre_state != self.DOWN):\n                            self.vec_flag.append(self.pre_state)\n                        if ((len(self.vec_flag)>10)or(self.pre_state == self.DOWN)or(self.pre_state == self.TRIGGER)):\n                            draw_img_np[:self.bin_height,:self.bin_width,:] = self.xia_argb\n                            self.cur_state = self.DOWN\n                    else:                                                                                   # \u624b\u6307\u5411\u5de6(\u5b9e\u9645\u65b9\u5411)\n                        if (self.pre_state != self.LEFT):\n                            self.vec_flag.append(self.pre_state)\n                        if ((len(self.vec_flag)>10)or(self.pre_state == self.LEFT)or(self.pre_state == self.TRIGGER)):\n                            draw_img_np[:self.bin_width,:self.bin_height,:] = self.zuo_argb\n                            self.cur_state = self.LEFT\n                    self.m_start = time.time_ns()\n            self.his_logit = []\n        else:\n            idx,avg_logit=output1,output2[0]\n            if (self.cur_state == self.UP):\n                draw_img_np[:self.bin_height,:self.bin_width,:] = self.shang_argb\n                if ((idx==15) or (idx==10)):\n                    self.vec_flag.clear()\n                    if (((avg_logit[idx] >= 0.7) and (len(self.his_logit) >= 2)) or ((avg_logit[idx] >= 0.3) and (len(self.his_logit) >= 4))):\n                        self.s_start = time.time_ns()\n                        self.cur_state = self.TRIGGER\n                        self.draw_state = self.DOWN\n                        self.history = [2]\n                    self.pre_state = self.UP\n                elif ((idx==25)or(idx==26)) :\n                    self.vec_flag.clear()\n                    if (((avg_logit[idx] >= 0.4) and (len(self.his_logit) >= 2)) or ((avg_logit[idx] >= 0.3) and (len(self.his_logit) >= 3))):\n                        self.s_start = time.time_ns()\n                        self.cur_state = self.TRIGGER\n                        self.draw_state = self.MIDDLE\n                        self.history = [2]\n                    self.pre_state = self.MIDDLE\n                else:\n                    self.his_logit.clear()\n            elif (self.cur_state == self.RIGHT):\n                draw_img_np[:self.bin_width,:self.bin_height,:] = self.you_argb\n                if  ((idx==16)or(idx==11)) :\n                    self.vec_flag.clear()\n                    if (((avg_logit[idx] >= 0.4) and (len(self.his_logit) >= 2)) or ((avg_logit[idx] >= 0.3) and (len(self.his_logit) >= 3))):\n                        self.s_start = time.time_ns()\n                        self.cur_state = self.TRIGGER\n                        self.draw_state = self.RIGHT\n                        self.history = [2]\n                    self.pre_state = self.RIGHT\n                else:\n                    self.his_logit.clear()\n            elif (self.cur_state == self.DOWN):\n                draw_img_np[:self.bin_height,:self.bin_width,:] = self.xia_argb\n                if  ((idx==18)or(idx==13)):\n                    self.vec_flag.clear()\n                    if (((avg_logit[idx] >= 0.4) and (len(self.his_logit) >= 2)) or ((avg_logit[idx] >= 0.3) and (len(self.his_logit) >= 3))):\n                        self.s_start = time.time_ns()\n                        self.cur_state = self.TRIGGER\n                        self.draw_state = self.UP\n                        self.history = [2]\n                    self.pre_state = self.DOWN\n                else:\n                    self.his_logit.clear()\n            elif (self.cur_state == self.LEFT):\n                draw_img_np[:self.bin_width,:self.bin_height,:] = self.zuo_argb\n                if ((idx==17)or(idx==12)):\n                    self.vec_flag.clear()\n                    if (((avg_logit[idx] >= 0.4) and (len(self.his_logit) >= 2)) or ((avg_logit[idx] >= 0.3) and (len(self.his_logit) >= 3))):\n                        self.s_start = time.time_ns()\n                        self.cur_state = self.TRIGGER\n                        self.draw_state = self.LEFT\n                        self.history = [2]\n                    self.pre_state = self.LEFT\n                else:\n                    self.his_logit.clear()\n\n            self.elapsed_time = round((time.time_ns() - self.m_start)/1000000)\n\n            if ((self.cur_state != self.TRIGGER) and (self.elapsed_time>2000)):\n                self.cur_state = self.TRIGGER\n                self.pre_state = self.TRIGGER\n\n        self.elapsed_ms_show = round((time.time_ns()-self.s_start)/1000000)\n        if (self.elapsed_ms_show<1000):\n            if (self.draw_state == self.UP):\n                draw_img.draw_arrow(1068,330,1068,130, (255,170,190,230), thickness=13)                             # \u5224\u65ad\u4e3a\u5411\u4e0a\u6325\u52a8\u65f6\uff0c\u753b\u4e00\u4e2a\u5411\u4e0a\u7684\u7bad\u5934\n                draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,32,"\u5411\u4e0a")\n            elif (self.draw_state == self.RIGHT):\n                draw_img.draw_arrow(1290,540,1536,540, (255,170,190,230), thickness=13)                             # \u5224\u65ad\u4e3a\u5411\u53f3\u6325\u52a8\u65f6\uff0c\u753b\u4e00\u4e2a\u5411\u53f3\u7684\u7bad\u5934\n                draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,32,"\u5411\u53f3")\n            elif (self.draw_state == self.DOWN):\n                draw_img.draw_arrow(1068,750,1068,950, (255,170,190,230), thickness=13)                             # \u5224\u65ad\u4e3a\u5411\u4e0b\u6325\u52a8\u65f6\uff0c\u753b\u4e00\u4e2a\u5411\u4e0b\u7684\u7bad\u5934\n                draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,32,"\u5411\u4e0b")\n            elif (self.draw_state == self.LEFT):\n                draw_img.draw_arrow(846,540,600,540, (255,170,190,230), thickness=13)                               # \u5224\u65ad\u4e3a\u5411\u5de6\u6325\u52a8\u65f6\uff0c\u753b\u4e00\u4e2a\u5411\u5de6\u7684\u7bad\u5934\n                draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,32,"\u5411\u5de6")\n            elif (self.draw_state == self.MIDDLE):\n                draw_img.draw_circle(320,240,100, (255,170,190,230), thickness=2, fill=True)                       # \u5224\u65ad\u4e3a\u4e94\u6307\u634f\u5408\u624b\u52bf\u65f6\uff0c\u753b\u4e00\u4e2a\u5b9e\u5fc3\u5706\n                draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,32,"\u4e2d\u95f4")\n        else:\n            self.draw_state = self.TRIGGER\n        pl.osd_img.copy_from(draw_img)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"lcd"\n    display_mode="lcd"\n    display_size=[640,480]\n    # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    hand_det_kmodel_path="/sdcard/examples/kmodel/hand_det.kmodel"\n    # \u624b\u90e8\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n    hand_kp_kmodel_path="/sdcard/examples/kmodel/handkp_det.kmodel"\n    # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n    gesture_kmodel_path="/sdcard/examples/kmodel/gesture.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    rgb888p_size=[1024,768]\n    hand_det_input_size=[512,512]\n    hand_kp_input_size=[256,256]\n    gesture_input_size=[224,224]\n    confidence_threshold=0.2\n    nms_threshold=0.5\n    labels=["hand"]\n    anchors = [26,27, 53,52, 75,71, 80,99, 106,82, 99,134, 140,113, 161,172, 245,276]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    sensor = Sensor(width=1280, height=960) # \u6784\u5efa\u6444\u50cf\u5934\u5bf9\u8c61\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create(sensor=sensor)  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    # \u81ea\u5b9a\u4e49\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\u5b9e\u4f8b\n    dg=DynamicGesture(hand_det_kmodel_path,hand_kp_kmodel_path,gesture_kmodel_path,det_input_size=hand_det_input_size,kp_input_size=hand_kp_input_size,gesture_input_size=gesture_input_size,labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                 # \u83b7\u53d6\u5f53\u524d\u5e27\n                output1,output2=dg.run(img)        # \u63a8\u7406\u5f53\u524d\u5e27\n                # print(output1, output2)            # \u6253\u5370\u7ed3\u679c\uff1f\n                dg.draw_result(pl,output1,output2) # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                    # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        dg.hand_det.deinit()\n        dg.hand_kp.deinit()\n        dg.dg.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.p,{children:"\u53ef\u4ee5\u770b\u5230\u9996\u5148\u662f\u5b9a\u4e49\u663e\u793a\u6a21\u5f0f\u3001\u56fe\u50cf\u5927\u5c0f\u3001\u6a21\u578b\u76f8\u5173\u7684\u4e00\u4e9b\u53d8\u91cf\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u63a5\u7740\u662f\u901a\u8fc7\u521d\u59cb\u5316PipeLine\uff0c\u8fd9\u91cc\u4e3b\u8981\u521d\u59cb\u5316sensor\u548cdisplay\u6a21\u5757\uff0c\u914d\u7f6e\u6444\u50cf\u5934\u8f93\u51fa\u4e24\u8def\u4e0d\u540c\u7684\u683c\u5f0f\u548c\u5927\u5c0f\u7684\u56fe\u50cf\uff0c\u4ee5\u53ca\u8bbe\u7f6e\u663e\u793a\u6a21\u5f0f\uff0c\u5b8c\u6210\u521b\u5efaPipeLine\u5b9e\u4f8b\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u7136\u540e\u8c03\u7528\u81ea\u5b9a\u4e49DynamicGesture\u7c7b\u6784\u5efa\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u7684\u4efb\u52a1\uff0cDynamicGesture\u7c7b\u4f1a\u901a\u8fc7\u8c03\u7528HandDetApp\u7c7b\u3001HandKPClassApp\u7c7b\u548cHandKPClassApp\u7c7b\u5b8c\u6210\u5bf9AIBase\u63a5\u53e3\u7684\u521d\u59cb\u5316\u4ee5\u53ca\u4f7f\u7528Ai2D\u63a5\u53e3\u7684\u65b9\u6cd5\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u6a21\u578b\u3001\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u578b\u548c\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u8f93\u5165\u56fe\u50cf\u7684\u9884\u5904\u7406\u65b9\u6cd5\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u6700\u540e\u5728\u4e00\u4e2a\u5faa\u73af\u4e2d\u4e0d\u65ad\u5730\u83b7\u53d6\u6444\u50cf\u5934\u8f93\u51fa\u7684RGBP888\u683c\u5f0f\u7684\u56fe\u50cf\u5e27\uff0c\u7136\u540e\u4f9d\u6b21\u5c06\u56fe\u50cf\u8f93\u5165\u5230\u624b\u638c\u68c0\u6d4b\u6a21\u578b\u3001\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u578b\u548c\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u624b\u638c\u68c0\u6d4b\u6a21\u578b\u3001\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u578b\u53ef\u4ee5\u83b7\u53d6\u624b\u638c\u7684\u5173\u952e\u4fe1\u606f\uff0c\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u53ef\u4ee5\u83b7\u53d6\u624b\u7684\u72b6\u4f53\uff0c\u901a\u8fc7\u4e09\u4e2a\u6a21\u578b\u7684\u914d\u5408\u4f7f\u7528\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u624b\u7684\u5173\u952e\u70b9\u68c0\u6d4b\u4ee5\u53ca\u624b\u7684\u52a8\u4f5c\u4fe1\u606f\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u7ed3\u679c\u7ed8\u5236\u5230\u56fe\u50cf\u4e0a\uff0c\u5e76\u5728LCD\u4e0a\u663e\u793a\u56fe\u50cf\u3002"}),"\n",(0,i.jsx)(n.h2,{id:"\u8fd0\u884c\u9a8c\u8bc1",children:"\u8fd0\u884c\u9a8c\u8bc1"}),"\n",(0,i.jsx)(n.p,{children:"\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5b9e\u73b0\u4e86\u4e94\u79cd\u52a8\u6001\u624b\u52bf\u7684\u8bc6\u522b\uff0c\u4e94\u79cd\u624b\u52bf\u5305\u62ec\uff1a\u4e0a\u6325\u624b\u3001\u4e0b\u6325\u624b\u3001\u5de6\u6325\u624b\u3001\u53f3\u6325\u624b\u3001\u624b\u6307\u634f\u5408\u4e94\u4e2a\u624b\u52bf\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u5c06K230D BOX\u5f00\u53d1\u677f\u8fde\u63a5CanMV IDE\uff0c\u70b9\u51fbCanMV IDE\u4e0a\u7684\u201c\u5f00\u59cb(\u8fd0\u884c\u811a\u672c)\u201d\u6309\u94ae\u540e\uff0c\u5c06\u6444\u50cf\u5934\u5bf9\u51c6\u624b\u638c\uff0c\u8ba9\u5176\u91c7\u96c6\u5230\u624b\u638c\u56fe\u50cf\uff0c\u5148\u8ba9\u624b\u6307\u5411\u4e0a\uff0c\u6b64\u65f6\u56fe\u50cf\u5de6\u4e0a\u89d2\u51fa\u73b0\u5411\u4e0a\u7684\u624b\u52bf\uff0c\u7136\u540e\u518d\u5c06\u624b\u6307\u634f\u5408\uff0c\u6b64\u65f6\u5c4f\u5e55\u4e2d\u95f4\u63d0\u793a\u201c\u4e2d\u95f4\u201d\uff0c\u8868\u793a\u624b\u6307\u634f\u5408\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"01",src:s(98864).A+"",width:"516",height:"391"})}),"\n",(0,i.jsx)(n.p,{children:"\u5148\u5c06\u624b\u6307\u671d\u4e0a\uff0c\u56fe\u50cf\u5de6\u4e0a\u89d2\u51fa\u73b0\u5411\u4e0a\u7684\u624b\u52bf\uff0c\u7136\u540e\u518d\u5c06\u624b\u6307\u5411\u4e0b\u7ffb\uff0c\u6b64\u65f6\u5c4f\u5e55\u4e2d\u95f4\u63d0\u793a\u201c\u5411\u4e0b\u201d\uff0c\u8868\u793a\u4e0b\u6325\u624b\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"01",src:s(5113).A+"",width:"517",height:"391"})}),"\n",(0,i.jsxs)(n.p,{children:["\u5176\u4ed6\u624b\u52bf\u70b9\u51fb",(0,i.jsx)(n.a,{href:"https://developer.canaan-creative.com/api/model/132/cover",children:"\u52a8\u6001\u624b\u52bf\u56fe"})]})]})}function o(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},98864:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/64-0422326545c746969e9a32a36475a7ee.png"}}]);